<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>vgg: Overview</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">vgg
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Overview </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This program trains <a class="el" href="structVGG.html" title="VGG network. ">VGG</a> network with training data from a specified file.</p>
<h1>Code: <a class="el" href="structVGG.html" title="VGG network. ">VGG</a> </h1>
<ul>
<li><a class="el" href="structVGG.html" title="VGG network. ">VGG</a>: <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</a></li>
<li>published at ICLR 2015 <a href="https://www.iclr.cc/archive/www/doku.php%3Fid=iclr2015:main.html">https://www.iclr.cc/archive/www/doku.php%3Fid=iclr2015:main.html</a></li>
</ul>
<h1>Dataset: CIFAR-10 </h1>
<ul>
<li>on IST Cluster, it's in /home/tau/cifar10</li>
<li>original source: <a href="http://www.cs.toronto.edu/~kriz/cifar.html">http://www.cs.toronto.edu/~kriz/cifar.html</a><ul>
<li>download "CIFAR-10 binary version (suitable for C programs)" if you download yourself</li>
</ul>
</li>
</ul>
<h1>Compile: </h1>
<div class="fragment"><div class="line">$ make</div><div class="line">g++  -O3 -DARRAY_INDEX_CHECK=0 -DMAX_BATCH_SIZE=64 -Wall -Wextra -Wno-strict-overflow -march=native -o vgg.g++ vgg.cc</div><div class="line">nvcc  -O3 -DARRAY_INDEX_CHECK=0 -DMAX_BATCH_SIZE=64  --generate-code arch=compute_60,code=sm_60 --generate-code arch=compute_70,code=sm_70 --compiler-options=-mavx2 -Xptxas -O3,-v -x cu -o vgg.nvcc vgg.cc</div></div><!-- fragment --><p>(make sure you have -O3 or -O0 -g in the command line, depending on whether you want to measure performance or debug)</p>
<h1>Run: </h1>
<ul>
<li>Make sure you have cifar-10-batches-bin/data_batch_1.bin file under the current directory. On IST cluster, you can do so by <div class="fragment"><div class="line">$ ln -s /home/tau/cifar10/cifar-10-batches-bin</div></div><!-- fragment --></li>
</ul>
<p>You may download the data from the original source if you like.</p>
<h2>CPU: </h2>
<div class="fragment"><div class="line">$ ./vgg.g++ [options]  # on login node. do not run for a long time</div><div class="line">$ srun -p big -t 0:20:00 ./vgg.g++ [options]</div></div><!-- fragment --><h2>GPU: </h2>
<div class="fragment"><div class="line">$ ./vgg.nvcc   # on login node. do not run for a long time</div><div class="line">$ srun -p p -t 0:20:00 --gres gpu:1 ./vgg.nvcc [options]</div><div class="line">$ srun -p v -t 0:20:00 --gres gpu:1 ./vgg.nvcc [options]</div></div><!-- fragment --><h1>Options </h1>
<h2>Verbosity (-v,&ndash;verbose) </h2>
<p>Give -v 2 option and you will see the progress more frequently. You can also know which functions are taking much time.</p>
<h2>Execution log (&ndash;log) </h2>
<p>Detailed execution records are saved into a file (default: vgg.log). You can specify the filename with &ndash;log option. When you execute many instances concurrently, make sure you specify a unique log file to each process.</p>
<h2>Batch size (-b,&ndash;batch_sz) </h2>
<p>For training, it repeats taking a number of samples and updating the model parameters (weights) to the direction that decreases the loss (the difference between the model prediction and the true label). In each iteration, it takes a number of samples specified by &ndash;batch_sz (-b). This number is called the mini-batch size. The default is MAX_BATCH_SIZE specified by a compile-time option -DMAX_BATCH_SIZE=N. For example, "g++ -DMAX_BATCH_SIZE=64 ... vgg.cc" sets the mini-batch size to 64. A usual value is 64 but you may consider changing it for performance tuning. Note that MAX_BATCH_SIZE affects the memory footprint. An instance of <a class="el" href="structVGG.html" title="VGG network. ">VGG</a> object holds all intermediate data within the instance and its size is roughly proportional to MAX_BATCH_SIZE. Note that specifying a small batch size at runtime (via &ndash;batch_sz) does not change the size of an instance.</p>
<p>The batch size significantly affects the time of a single iteration, especially in an unoptimized baseline code. The baseline code will take a time proportional to the batch size for a single iteration.</p>
<p>For a quick experiment, you will want to make it small (e.g., -b 1).</p>
<p>For easier debugging, you may also want to consider compiling the program with -DMAX_BATCH_SIZE=1</p>
<h2>The number of iterations (-m,&ndash;iters) </h2>
<p>-m option specifies the number of iterations. Default 20.</p>
<p>For a quick experiment, you will want to make both batch size and iterations small (e.g., -m 1 -b 1).</p>
<h2>Turn on/off dropout (&ndash;dropout 1/0) </h2>
<p>There is a layer called dropout, which randomly turns off (zeros) output of the previous layer (i.e., output_i = 0 with some probability and input_i otherwise). <a class="el" href="structDropout.html" title="dropout layer ">Dropout</a> is generally believed to improve generalization. You may turn off this feature during development, to make sure the network is exactly the same throughout iterations.</p>
<p>You can turn off dropout by &ndash;droput 0 (default: 1)</p>
<h2>Fix a batch (&ndash;single_batch 1) </h2>
<p>During development, you may want to repeat processing the same mini-batch again and again, to make sure that the network is at least adjusting to the particular mini-batch. Combine this with &ndash;dropout 0 (and perhaps with a small batch size, like -b 16 or even -b 1). In those cases the loss should steadily decrease over iterations. If it does not happen, suspect your bug, particularly in your backward phase.</p>
<h2>Data file (-d, &ndash;start_data and &ndash;end_data) </h2>
<p>It reads data from the file specified by &ndash;cifar_data (-d) option (default: cifar-10-batches-bin/data_batch_1.bin). The original data can be obtained from <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a> (get "CIFAR-10 binary version (suitable for C programs)" or <a href="https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz">https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz</a>). It contains 5 datasets and each one has 10000 images.</p>
<p>If you want to use only a part of data, you can specify a range by &ndash;start_data and &ndash;end_data. e.g., &ndash;start_data 2000 &ndash;end_data 3000 uses only 1000 images, from the 2000th image to 2999th image.</p>
<h2>Data for training and validation (&ndash;validate_ratio and &ndash;validate_interval) </h2>
<p>A portion of data is reserved for validation and not used for training. The ratio of validation data relative to the whole data set is specified by &ndash;validate_ratio option (default: 0.1). Note that they are taken out from the data specified by &ndash;cifar_data, &ndash;start_data and &ndash;end_data. For example, &ndash;start_data 2000 &ndash;end_data 3000 &ndash;validate_ratio 0.1 uses (3000 - 2000) * 0.1 = 100 images for validation and leaves 900 images for training.</p>
<p>Fractions are rounded down. If the number of validation data becomes zero as a result, the validation won't be performed at all.</p>
<p>It occasionally evaluates the loss against the validation data. The frequency can be adjusted by &ndash;validate_interval option, which specifies the relative number of samples to process for training and that for validation. For example, if it is set to 5.0 and the number of data for validation is 100, it runs a validation for every 5.0 * 100 = 500 training samples processed. In other words, &ndash;validate_interval x sets the time to run a validation below 1/(1+x) of the total processing time.</p>
<h2>Learning rate </h2>
<p>In each iteration, the backward phase calculates the gradient of the averaged loss (the average taken over the mini-batch) with respect to all the weights of the network. The update phase that follows then changes the weights to the opposite direction of the gradient. Mathematically, we do </p><pre class="fragment">W = W - (η/B) ∂L/∂W
</pre><p>where L is the summation of the loss function over the mini batch and B the batch size (hence 1/B ∂L/∂W represents the gradient of the average loss with respect to weights W). We multiply it by a constant η, which is called a learning rate.</p>
<p>The reason why the above update will decrease the loss is as follows. In general, </p><pre class="fragment">L(W + ΔW) ≒ L(W) + ∂L/∂W・ΔW
</pre><p>holds, where ・ represents the inner product (summation of all component-wise products).</p>
<p>So if ΔW is taken as an opposite direction of ∂L/∂W (i.e., ΔW = -η∂L/∂W), then </p><pre class="fragment">L(W + ΔW) ≒ L(W) + ∂L/∂W・ΔW
           = L(W) + ∂L/∂W ・(-η∂L/∂W)
           = L(W) -η ∂L/∂W・∂L/∂W
           &lt; L(W)
</pre><p>The value of learning rate can be specified with &ndash;learnrate option. Default: 1.0e-2</p>
<p>The learning rate does not affect the time of each iteration, but may affect the time until convergence.</p>
<p>Since the first-order approximation </p><pre class="fragment">L(W + ΔW) ≒ L(W) + ∂L/∂W・ΔW
</pre><p>holds only for small ΔW, picking a too large η may break the very basic property that an update will decrease the loss.</p>
<p>On the other hand, the larger the value of η, the "faster" weights move to the opposite direction of the gradient and the faster L(w) decreases. You may be able to reach the same point with a fewer number of iterations.</p>
<p>In summary, choosing the right value for η is important but not necessarily easy.</p>
<p>Since our primary focus in the exercise is on optimizing each stage of computation, we are not concerned about the choice of η too much. Specifically, our (primary) performance criteria will be the average time to process a single sample.</p>
<h2>Change seeds </h2>
<p>There are a few places in which the program uses random numbers, namely,</p>
<ul>
<li>when it initializes weight parameters,</li>
<li>when it divides the input data into validation and training,</li>
<li>when it chooses training samples in each iteration and</li>
<li>when it uses which cells to dropout in a dropout layer</li>
</ul>
<p><em>ALL COMPONENTS BEHAVE DETERMINISTICALLY.</em> That is, if you repeat executions with the same configuration repeatedly, it starts from the same weight parameters, uses the same data for validation and training, picks up the same training data and drops out the same cells.</p>
<p>Unless your algorithm behave undeterministically, the results should be always the same. This should help you debug your code.</p>
<p>You can change these things by giving different seeds for each of these random number generators. Specifically,</p>
<ul>
<li>&ndash;weight_seeds changes initial weight parameters</li>
<li>&ndash;validate_seed changes which data are used for validation</li>
<li>&ndash;sample_seed changes which data are picked for training</li>
<li>&ndash;dropout_seed changes which cells are dropped out</li>
</ul>
<p>Simply give an arbitrary number to any of them to make sure your algorithm is not sensitive to any of them.</p>
<p>A general remark: when using a pseudo random number for a randomized algorithm such as stochastic gradient descent, <em>ALWAYS</em> give it a seed you chose and make it behave deterministically given the same seed. This is a tremendous help for debugging. After you have done initial debugging, you can test your algorithm across different sequences of numbers just by giving different seeds. Note that virtually all pseudo random number generators are deterministic after a seed is given. A random number generator without any seed generates different sequences every time simply because they use different seeds every time; when you do not give it a seed, it simply takes a value from an external source (e.g., the current time) and uses it as a seed. Nothing else is different. In this sense, there is almost no point in not giving a seed of your choice (give the current time as a seed if you want to purposefully make it behave differently each time). Without giving a seed, your algorithm can NEVER behave deterministically, which is a nightmare for debugging and the nightmare is nothing but unnecessary.</p>
<h2>GPU execution (-a gpu_base) </h2>
<p>The program compiled with nvcc, vgg.nvcc, supports GPU execution and this is the default behavior. vgg.nvcc also supports CPU execution with -a cpu_base option.</p>
<div class="fragment"><div class="line">$ vgg.nvcc -a cpu_base  # (1) baseline code on CPU </div><div class="line">$ vgg.nvcc -a gpu_base  # (2) baseline code on GPU </div><div class="line">$ vgg.nvcc              # (3) same as (2)</div><div class="line">$ vgg.g++  -a cpu_base  # (4) baseline code on CPU </div><div class="line">$ vgg.g++  -a gpu_base  # (5) error</div><div class="line">$ vgg.g++               # (6) same as (4)</div></div><!-- fragment --><p>Note that baseline code is neither vectorized nor parallelized. In particular, it uses only a single CUDA thread on GPU (!)</p>
<h2>Algorithm choice (-a) </h2>
<p>The -a option described above is an option that chooses an algorithm from available repertories. In the given code, only baseline algorithms for GPU and CPU are implemented. You probably want to make your implementation another available choice here.</p>
<h2>Controlled experiments </h2>
<p>After you did a bit of work, you want to make sure you got it done right. Especially, you may be afraid that you broke a function. To make sure the network is still functioning, you might want to do a small and controlled experiment.</p>
<p>You probably want to start with something like this.</p>
<div class="fragment"><div class="line">$ ./vgg.gcc --dropout 0 --single_batch 1 -b 1</div></div><!-- fragment --><p>They together (1) turn off dropout to avoid fluctuating losses across iterations due to the changing network (&ndash;dropout 0), (2) process the same mini-batch at every iteration to avoid fluctuating losses due to different data in different iterations, and (3) make the mini-batch size extremely small (1, in this particular case) to have a quick turn around time.</p>
<p>Here is a sample output. With the default value of learning rate (1.0e-2), the loss seems decreasing too quickly.</p>
<div class="fragment"><div class="line">$ ./vgg.gcc --dropout 0 --single_batch 1 -b 1</div><div class="line">=== train 0 - 1 ===</div><div class="line">train loss = 2.064959049</div><div class="line">=== train 1 - 2 ===</div><div class="line">train loss = 0.002221737</div><div class="line">=== train 2 - 3 ===</div><div class="line">train loss = 0.002113967</div><div class="line">=== train 3 - 4 ===</div><div class="line">train loss = 0.002022009</div><div class="line">=== train 4 - 5 ===</div><div class="line">train loss = 0.001938014</div><div class="line">=== train 5 - 6 ===</div><div class="line">train loss = 0.001864006</div><div class="line">=== train 6 - 7 ===</div><div class="line">train loss = 0.001799989</div><div class="line">=== train 7 - 8 ===</div><div class="line">train loss = 0.001746916</div><div class="line">  ...</div></div><!-- fragment --><p>Remember that this will repeat processing only a single sample and the "train loss" refers to the loss against this particular sample. If the loss does not decrease, you are very likely to have introduced a bug in your gradient calculation (backward) or somewhere else.</p>
<p>You may play with small learning rates to see that it will keep decreasing for a larger number of iterations.</p>
<div class="fragment"><div class="line">$ ./vgg.gcc --dropout 0 --single_batch 1 -b 1 --learnrate 1.0e-3</div><div class="line">=== train 0 - 1 ===</div><div class="line">train loss = 2.064959049</div><div class="line">=== train 1 - 2 ===</div><div class="line">train loss = 0.937229633</div><div class="line">=== train 2 - 3 ===</div><div class="line">train loss = 0.503207982</div><div class="line">=== train 3 - 4 ===</div><div class="line">train loss = 0.290159106</div><div class="line">=== train 4 - 5 ===</div><div class="line">train loss = 0.209925532</div><div class="line">=== train 5 - 6 ===</div><div class="line">train loss = 0.163231462</div><div class="line">  ...</div></div><!-- fragment --><h1>Guide for development </h1>
<h2>Source code structure </h2>
<ul>
<li><a class="el" href="vgg_8cc.html">vgg.cc</a> &ndash; the main file</li>
<li>include/<ul>
<li><a class="el" href="vgg__util_8h.html" title="VGG utility functions/classes. ">vgg_util.h</a> &ndash; trivial utilities</li>
<li><a class="el" href="cuda__util_8h.html" title="small utility functions for cuda ">cuda_util.h</a> &ndash; helpers for CUDA</li>
<li><a class="el" href="vgg__arrays_8h.html" title="vectors, matrices, and multi-dimensional arrays ">vgg_arrays.h</a> &ndash; vectors, matrix and multidimensional tensors</li>
<li><a class="el" href="cifar_8h.html" title="cifar dataset handling ">cifar.h</a> &ndash; data loader (primitive layers)</li>
<li><a class="el" href="convolution_8h.html" title="convolution layer ">convolution.h</a> &ndash; convolution</li>
<li><a class="el" href="batchnormalization_8h.html" title="batch normalization layer ">batchnormalization.h</a> &ndash; batch normalization</li>
<li><a class="el" href="relu_8h.html" title="rectified linear activation layer (relu(x) = max(0,x)) ">relu.h</a> &ndash; rectified linear activation</li>
<li><a class="el" href="dropout_8h.html" title="dropout layer ">dropout.h</a> &ndash; dropout</li>
<li><a class="el" href="linear_8h.html" title="linear (fully connected) layer ">linear.h</a> &ndash; linear (or fully connected) layer</li>
<li><a class="el" href="maxpooling_8h.html" title="max pooling layer ">maxpooling.h</a> &ndash; max pooling</li>
<li><a class="el" href="softmaxcrossentropy_8h.html" title="softmax + cross entropy layer ">softmaxcrossentropy.h</a> &ndash; softmax + cross entropy (composite layers)</li>
<li><a class="el" href="block_8h.html" title="a block of three layers (convolution; batch normalization; relu) ">block.h</a> &ndash; convolution; batch normalization; relu</li>
<li><a class="el" href="vgg_8h.html" title="VGG network. ">vgg.h</a> &ndash; the entire <a class="el" href="structVGG.html" title="VGG network. ">VGG</a></li>
</ul>
</li>
</ul>
<p>The main function in <a class="el" href="vgg_8cc.html">vgg.cc</a> instantiates a <a class="el" href="structVGG.html" title="VGG network. ">VGG</a> network, which is defined in <a class="el" href="vgg_8h.html" title="VGG network. ">vgg.h</a>. It repeats processing training data, occasionally processing validation data.</p>
<p>Each layer defines a class whose name is similar to the file name. e.g., <a class="el" href="convolution_8h.html" title="convolution layer ">convolution.h</a> defines <a class="el" href="structConvolution2D.html" title="convolution of images ">Convolution2D</a> class.</p>
<p>All classes for primitive and composite layers have two important functions, among others.</p><ul>
<li>forward &ndash; take an input from the previous (downstream) layer and computes its output</li>
<li>backward &ndash; take a gradient of loss wrt the upstream layer and computes the gradient wrt its input and weights</li>
</ul>
<p>In addition, classes that have parameters (convolution, linear and batchnormalization) have another function.</p>
<ul>
<li>update &ndash; take a learning rate parameter and update its weights, using the gradient computed in the backward phase.</li>
</ul>
<hr/>
<p> YOUR MAIN JOB WILL BE TO IMPLEMENT A HIGH PERFORMANCE VERSION OF THESE FUNCTIONS. </p><hr/>
<p>You eventually want to work on all seven files (<a class="el" href="convolution_8h.html" title="convolution layer ">convolution.h</a>, <a class="el" href="batchnormalization_8h.html" title="batch normalization layer ">batchnormalization.h</a>, <a class="el" href="relu_8h.html" title="rectified linear activation layer (relu(x) = max(0,x)) ">relu.h</a>, <a class="el" href="dropout_8h.html" title="dropout layer ">dropout.h</a>, <a class="el" href="linear_8h.html" title="linear (fully connected) layer ">linear.h</a>, <a class="el" href="maxpooling_8h.html" title="max pooling layer ">maxpooling.h</a>, <a class="el" href="softmaxcrossentropy_8h.html" title="softmax + cross entropy layer ">softmaxcrossentropy.h</a>) but you can work incrementally. You can make one layer faster while leaving all others intact. You can know which functions are taking much time by -v 2 option.</p>
<h2>Stepping through the code using gdb (or cuda-gdb) </h2>
<p>When working on details, you want to step through the code using gdb (or cuda-gdb to step through GPU code). They also help you get an idea about how things work. For that, compile the code with -O0 -g. Also add -Xptxas -O0 if you compile with nvcc.</p>
<h2>The structure of the baseline implementations (and switching between different algorithms) </h2>
<p>As I mentioned above, functions you will primarily be working on are forward and backward functions on the seven classes and update functions for the three classes. Each of them has a structure to switch between GPU code and CPU code (currently, a single execution can run either entirely on CPU or entirely on GPU; you cannot have some layers executed by CPU and others on GPU in the same execution). Let's look at the forward function of Convolution class, for example.</p>
<p>The member function named "forward" is the entry point of the forwarding phase. It only executes a switch statement to decide which implementation to use (cpu or gpu in the baseline code).</p>
<div class="fragment"><div class="line">  array4&lt;maxB,OC,H,W&gt;&amp; forward(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">    if (opt.verbose&gt;=2) { print_start(); }</div><div class="line">    tsc_t t0 = get_tsc();</div><div class="line">    switch (opt.algo) {</div><div class="line">      /* add case for your implementations here */</div><div class="line">    case algo_cpu_base:</div><div class="line">      forward_cpu(x); break;</div><div class="line">#if __NVCC__</div><div class="line">    case algo_gpu_base:</div><div class="line">      forward_gpu(x); break;</div><div class="line">#endif</div><div class="line">    default:</div><div class="line">      if (opt.gpu_algo) {</div><div class="line">#if __NVCC__</div><div class="line">        forward_gpu(x);</div><div class="line">#else</div><div class="line">        err_gpu_algo_no_gpu(opt.algo_s);</div><div class="line">#endif</div><div class="line">      } else {</div><div class="line">        forward_cpu(x);</div><div class="line">      }        </div><div class="line">    }</div><div class="line">    tsc_t t1 = get_tsc();</div><div class="line">    if (opt.verbose&gt;=2) { print_end(t0, t1); }</div><div class="line">    return y;</div><div class="line">  }</div></div><!-- fragment --><p>Depending on the algorithm chosen at the command line (-a option), it calls either forward_cpu or forward_gpu. The former simply calls another function, forward_base, which does the real job.</p>
<div class="fragment"><div class="line">void forward_cpu(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  forward_base(x);</div><div class="line">}</div></div><!-- fragment --><p>The latter calls into a GPU code. Since nvcc does not allow a class member function to be a global function (a GPU function callable from a host), we need to define a global function outside the class (forward_global), which then calls back a member function (forward_dev). This is the baseline implementation of forward_gpu.</p>
<div class="fragment"><div class="line">void forward_gpu(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  launch_and_sync((forward_global&lt;&lt;&lt;1,1&gt;&gt;&gt;(dev, x.dev)));</div><div class="line">}</div></div><!-- fragment --><p>The global function, forward_global, is defined outside the class as follows. Note that it launches only a single CUDA-thread, something you definitely want to do differently in your high performance version.</p>
<div class="fragment"><div class="line">template&lt;idx_t maxB,idx_t IC,idx_t H,idx_t W,idx_t K,idx_t OC&gt;</div><div class="line">__global__ void forward_global(Convolution2D&lt;maxB,IC,H,W,K,OC&gt;* dev,</div><div class="line">                               array4&lt;maxB,IC,H,W&gt;* x_dev) {</div><div class="line">  dev-&gt;forward_dev(*x_dev);</div><div class="line">}</div></div><!-- fragment --><p>The member function forward_dev actually calls the same forward_base function that does the real job.</p>
<div class="fragment"><div class="line">__device__ __host__ </div><div class="line">void forward_base(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  ... do the real job ...</div><div class="line">}</div></div><!-- fragment --><p>This same pattern appears for backward and update too. In this way, the baseline code shares the same piece of code between CPU and GPU. The trick makes sense only for the baseline code. In your high performance implementations, you are probably going to have separate pieces of code for CPU and GPU anyways.</p>
<h2>How to add your implementation </h2>
<p>Here is how you change the code when working on a new implementation. As already mentioned, there are two implementations already in place, cpu_base and gpu_base.</p>
<p>Before starting the real work, there are some work for preparation.</p>
<ul>
<li>Come up with a name of the new implementation. Let's say it is cpu_ultra_fast (in reality, you want to have a name that better represents what it does, like cpu_simd).</li>
<li>Add a new symbol to the enum algo_t defined in <a class="el" href="vgg__util_8h.html" title="VGG utility functions/classes. ">vgg_util.h</a> <div class="fragment"><div class="line">typedef enum {</div><div class="line">  algo_cpu_base,</div><div class="line">  algo_gpu_base,</div><div class="line">  /* add your new algorithm here (name it arbitrarily) */</div><div class="line"></div><div class="line">  algo_cpu_ultra_fast, &lt;----  YOU ADD THIS</div><div class="line"></div><div class="line">  algo_invalid,</div><div class="line">} algo_t;</div></div><!-- fragment --></li>
<li>Change the parse_algo function right below it so that it recognizes the new name. Obviously, the baseline code recognizes only "cpu_base" and "gpu_base". You simply add an appropriate "else if" branch to handle your name.</li>
</ul>
<div class="fragment"><div class="line">algo_t parse_algo(const char * s) {</div><div class="line">  if (strcmp(s, &quot;cpu_base&quot;) == 0) {</div><div class="line">    return algo_cpu_base;</div><div class="line">  } else if (strcmp(s, &quot;gpu_base&quot;) == 0) {</div><div class="line">    return algo_gpu_base;</div><div class="line">  } else if (strcmp(s, &quot;cpu_ultra_fast&quot;) == 0) {  &lt;---- YOU ADD THIS</div><div class="line">    return algo_cpu_ultra_fast;                   &lt;---- YOU ADD THIS</div><div class="line">  } else {</div><div class="line">    return algo_invalid;</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><ul>
<li>You might also need to change the function algo_is_gpu so that the program correctly recognizes whether it is a CPU algorithm or a GPU algorithm. By default, it simply assumes all and only names starting with "gpu" are GPU algorithms. You need to change this only when your algorithm name does not conform to this convention (e.g., a GPU algorithm named "v100_only"). It will be a good idea to stick to the convention rather than modifying this function.</li>
</ul>
<div class="fragment"><div class="line">int algo_is_gpu(const char * s, algo_t a) {</div><div class="line">  (void)a;</div><div class="line">  if (strncmp(s, &quot;gpu&quot;, 3) == 0) {</div><div class="line">    return 1;</div><div class="line">  } else { </div><div class="line">    return 0;</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>At this point, the program at least recognizes your algorithm and calls GPU base code or CPU base code depending on your algorithm is a GPU algorithm or not (judged by algo_is_gpu function above). Recall that the switch statement falls back to forward_gpu or forward_cpu when the switch statement does not have a specific case for your algorithm.</p>
<p>Now you are ready to add a real implementation. Thanks to the structure just mentioned, you can do so incrementally (you do not have to implement all functions to get your version used). To start off, let's say you want to implement a forward function of <a class="el" href="structConvolution2D.html" title="convolution of images ">Convolution2D</a> class. The first thing you need to do is to add an appropriate case in the switch statement.</p>
<div class="fragment"><div class="line">array4&lt;maxB,OC,H,W&gt;&amp; forward(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  if (opt.verbose&gt;=2) { print_start(); }</div><div class="line">  tsc_t t0 = get_tsc();</div><div class="line">  switch (opt.algo) {</div><div class="line">    /* add case for your implementations here */</div><div class="line">  case algo_cpu_ultra_fast:</div><div class="line">    forward_cpu_ultra_fast(x); break;</div><div class="line"></div><div class="line">    ...</div><div class="line">  }</div><div class="line">  tsc_t t1 = get_tsc();</div><div class="line">  if (opt.verbose&gt;=2) { print_end(t0, t1); }</div><div class="line">  return y;</div><div class="line">}</div></div><!-- fragment --><p>Your real job is, of course, to implement forward_cpu_ultra_fast function. Use SIMD, OpenMP or whatever is necessary to make it faster. You probably start by copy-pasting the forward_base implementation.</p>
<p>If you work on GPU implementation, you need to implement two functions. Let's say your algorithm name is gpu_ultra_fast. After adding another case in the switch statement like this</p>
<div class="fragment"><div class="line">case algo_gpu_ultra_fast:</div><div class="line">  forward_gpu_ultra_fast(x); break;</div></div><!-- fragment --><p>your forward_gpu_ultra_fast should launch a global function with a good thread block size.</p>
<div class="fragment"><div class="line">void forward_gpu_ultra_fast(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  int block_sz = ...;</div><div class="line">  int num_blocks = ...;</div><div class="line">  launch_and_sync((forward_ultra_fast_global&lt;&lt;&lt;num_blocks,block_sz&gt;&gt;&gt;(dev, x.dev)));</div><div class="line">}</div></div><!-- fragment --><p>Next you define forward_ultra_fast_global function outside the class, near the beginning of the file. This will be a boilerplate code.</p>
<div class="fragment"><div class="line">template&lt;idx_t maxB,idx_t IC,idx_t H,idx_t W,idx_t K,idx_t OC&gt;</div><div class="line">__global__ void forward_ultra_fast_global(Convolution2D&lt;maxB,IC,H,W,K,OC&gt;* dev,</div><div class="line">                                          array4&lt;maxB,IC,H,W&gt;* x_dev) {</div><div class="line">  dev-&gt;forward_ultra_fast_dev(*x_dev);</div><div class="line">}</div></div><!-- fragment --><p>Finally, you define forward_ultra_fast_dev member function, which does the real job.</p>
<p>In forward_base function that is supposed to do a real job, you compute the output and put them in the 'y' variable, which is already defined for you as a member field. This convention is used throughout the program. All classes have a member field named 'y' to which you should put the results.</p>
<div class="fragment"><div class="line">__device__ __host__ </div><div class="line">void forward_base(array4&lt;maxB,IC,H,W&gt;&amp; x) {</div><div class="line">  idx_t B = x.B;</div><div class="line">  y.set_n_rows(B);</div><div class="line">  ...</div><div class="line">  for (idx_t b = 0; b &lt; B; b++) {       // samples</div><div class="line">    ...</div><div class="line">       ...</div><div class="line">          y(b,oc,i,j) = s;</div><div class="line">       ...</div><div class="line">    ...</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p>Similarly, a backward implementation is supposed to put the results into another member variable named 'gx' (∂L/∂x, gradients with respect to x) and 'gw' if the layer has weights.</p>
<p>There is one thing to note here. The input typically is an array (single- or multi-dimensional) whose primary (leftmost) index refers to a particular sample in a mini-batch. In the above example, x is a four dimensional array and thus has a type <a class="el" href="structarray4.html">array4&lt;maxB,IC,H,W&gt;</a>&amp;. maxB is a <em>compile time</em> constant you specified by -DMAX_BATCH_SIZE=xxx. <em>The actual number of samples in this array may be smaller and is passed via a field variable of the input.</em> You have to process only the actual number of samples passed in the array.</p>
<p>In this example, x.B has the actual number of rows in the array. Thus,</p><ul>
<li>the outermost loop iterates x.B number of times rather than maxB times. <div class="fragment"><div class="line">idx_t B = x.B;</div><div class="line"> ...</div><div class="line">for (idx_t b = 0; b &lt; B; b++) {       // samples</div></div><!-- fragment --></li>
<li>it also sets the actual number of rows in the output y, by doing <div class="fragment"><div class="line">idx_t B = x.B;</div><div class="line">y.set_n_rows(B);</div></div><!-- fragment --></li>
</ul>
<h2>Debugging a layer </h2>
<p>After you change implementation of a layer you will want to make sure you got it right. It may not happen immediately, however. Several mechanisms are in place to help you debug them efficiently.</p>
<h2>Catching basic coding errors </h2>
<p>First, after you change an implementation of a layer, make sure you turn a compile-time option -DARRAY_INDEX_CHECK=1 on. This will check array index every time you access an element of a vector, matrix or tensor. It will catch obvious errors such as looping with wrong bounds or indexing arrays with wrong variables.</p>
<div class="fragment"><div class="line">$ g++ ... -DARRAY_INDEX_CHECK=1 ... -o vgg.g++ vgg.cc</div></div><!-- fragment --><h2>Catching logical (mathematics) errors </h2>
<p>After you have a code that at least is not caught by array indexing errors, you now want to check if the code really does the job. The first command line you want to test your code with is this.</p>
<div class="fragment"><div class="line">$ ./vgg.gcc --dropout 0 --single_batch 1 -b 1 -a name_of_your_algorithm</div></div><!-- fragment --><p>Like I introduced already, this processes only a single sample repeatedly, without any dropout that would introduce different behaviors between iterations. The error thus should steadily decrease over iterations, in almost exactly the same pace with the baseline implementation. Try different learning rate (&ndash;learnrate 1.0e-3, 1.0e-4, etc.) and confirm they behave very similarly for each one.</p>
<p>Remember that both are doing exactly the same computation. There is a randomness in how it chooses samples, but <em>it is deterministic</em> as I already mentioned; the sample picked should be the same unless you change the seed of the random number generator (by &ndash;sample_seed). Were there any difference between the two implementations, it should indicate that your algorithm outputs results different from the other implementation for the same input. If the difference is slight, it may be indicating that the two has different rounding errors for computing mathematically equivalent expressions. In particular, summing up many numbers in a different order affect rounding errors very much. When you parallelize or vectorize your code, you almost certainly change the order in which numbers are accumulated. Therefore, a slight difference in the loss may not be worth looking into.</p>
<p>If you observe a significant change, you need to shoot down where you introduce a bug, for which you will want to debug a single layer at a time.</p>
<p>Each layer is implemented in a single header file.</p>
<ul>
<li><a class="el" href="batchnormalization_8h.html" title="batch normalization layer ">batchnormalization.h</a></li>
<li><a class="el" href="convolution_8h.html" title="convolution layer ">convolution.h</a></li>
<li><a class="el" href="dropout_8h.html" title="dropout layer ">dropout.h</a></li>
<li><a class="el" href="linear_8h.html" title="linear (fully connected) layer ">linear.h</a></li>
<li><a class="el" href="maxpooling_8h.html" title="max pooling layer ">maxpooling.h</a></li>
<li><a class="el" href="relu_8h.html" title="rectified linear activation layer (relu(x) = max(0,x)) ">relu.h</a></li>
<li><a class="el" href="softmaxcrossentropy_8h.html" title="softmax + cross entropy layer ">softmaxcrossentropy.h</a></li>
</ul>
<p>Each header file actually contains an entry point function so that it can compile and run alone. For example, <a class="el" href="convolution_8h.html" title="convolution layer ">convolution.h</a> has a function convolution_main that runs only a convolution. Therefore, if you include this file from any C++ file and compile it with -Dconvolution_main=main, you get an executable that only runs that layer.</p>
<p>Indeed,</p>
<div class="fragment"><div class="line">$ cd include</div><div class="line">$ make</div></div><!-- fragment --><p>builds all such executables. You will obtain batchnormalization.{float,double}.{g++,nvcc}, etc.</p>
<p>The entry point function checks if the gradients obtained by forward/backward computation indeed approximate the change of the output value. Specifically, let's say we have a layer implementing a function F(W, X). We check if the following approximation holds. </p><pre class="fragment">F(W + ΔW/2, X + ΔX/2) - F(W - ΔW/2, X - ΔX/2) ≒ ∂F/∂W・ΔW + ∂F/∂X・ΔX
</pre><p>(There are many layers that do not have weight parameters. For such layers, we simply check if F(X + ΔX/2) - F(X - ΔX/2) ≒ ∂F/∂X・ΔX holds).</p>
<p>In implementation terms, we</p>
<ul>
<li>generate inputs (X) and weights (W) randomly</li>
<li>generate small changes to inputs (ΔX) and weights (ΔW) randomly</li>
<li>perform forward and backward computation to obtain ∂F/∂W and ∂F/∂X and thus to obtain ∂F/∂W・ΔW + ∂F/∂X・ΔX</li>
<li>apply changes to X and W to obtain X±ΔX and W±ΔW</li>
<li>perform forward computation on both of them, to obtain F(W + ΔW/2, X + ΔX/2) and F(W - ΔW/2, X - ΔX/2)</li>
<li>compare F(W + ΔW/2, X + ΔX/2) and F(W - ΔW/2, X - ΔX/2) and ∂F/∂W・ΔW + ∂F/∂X・ΔX and report their relative difference. The relative difference between A and B is |A-B|/max(|A|,|B|)</li>
</ul>
<p>Here is an output of the linear layer.</p>
<div class="fragment"><div class="line">$ ./linear.float.g++ -b 1</div><div class="line">==== 0 ====</div><div class="line">|∂L/∂x|   = 2.067968130</div><div class="line">|dx|      = 0.001285600</div><div class="line">∂L/∂x・dx = -0.000187458</div><div class="line">|∂L/∂w|   = 27.588932037</div><div class="line">|dw|      = 0.004125366</div><div class="line">∂L/∂w・dw = 0.000541298</div><div class="line">L- = -0.982513964</div><div class="line">L  = -0.982336760</div><div class="line">L+ = -0.982159197</div><div class="line">A = ∂L/∂x・dx + ∂L/∂w・dw = 0.000353840</div><div class="line">B = ΔL = 0.000354767</div><div class="line">relative error = |A-B|/max(|A|,|B|) = 0.002611878</div><div class="line">==== 1 ====</div><div class="line">|∂L/∂x|   = 2.037896156</div><div class="line">|dx|      = 0.001299710</div><div class="line">    ...</div><div class="line"></div><div class="line"></div><div class="line">max relative error = 0.009731923</div><div class="line">avg relative error = 0.001512904</div></div><!-- fragment --><p>In the end of the execution, it reports that the maximum and average relative errors are 0.009731923 and 0.001512904, respectively.</p>
<p>Note that linear layer implements a linear function, for which an equation </p><pre class="fragment">F(W + ΔW/2, X + ΔX/2) - F(W - ΔW/2, X - ΔX/2) = ∂F/∂W・ΔW + ∂F/∂X・ΔX
</pre><p>should strictly hold if all elementary computations are done without rounding errors. Any error should be due to rounding errors, which should be small if you are not accumulating too many numbers of numbers of significantly different magnitudes.</p>
<p>If the reported relative error is small enough, it means that moving the weights to the opposite direction of the computed gradient should decrease the loss function, which is the very purpose of the optimization process. As long as this holds, you do not have be concerned too much about the difference to the baseline code, which has its own rounding errors.</p>
<p>How small is small enough? It actually depends on layers and the type of floating point numbers. Especially when using a single precision floating point numbers (executables *.float.*} do so), rounding errors easily become significant. Average relative errors of 10% or even 30% do not necessarily indicate a bug. Double precision numbers are much less prone to rounding errors. For the purpose of checking if your code faithfully computes what it should compute, consider testing the double-precision version of it (*.double.*), which should report tiny relative errors. Here are tables summarizing the maximum/average errors for a single sample.</p>
<table class="doxtable">
<tr>
<th>layer </th><th align="center">max (SP) </th><th align="center">avg (SP) </th><th align="center">max (DP) </th><th align="center">max (DB)  </th></tr>
<tr>
<td>batchnormalization </td><td align="center">0.321599394 </td><td align="center">0.047267061 </td><td align="center">0.000000012 </td><td align="center">0.000000001 </td></tr>
<tr>
<td>convolution </td><td align="center">0.005766520 </td><td align="center">0.001314429 </td><td align="center">0.000000000 </td><td align="center">0.000000000 </td></tr>
<tr>
<td>dropout </td><td align="center">0.927670479 </td><td align="center">0.083410397 </td><td align="center">0.000000003 </td><td align="center">0.000000000 </td></tr>
<tr>
<td>linear </td><td align="center">0.009731923 </td><td align="center">0.001512904 </td><td align="center">0.000000000 </td><td align="center">0.000000000 </td></tr>
<tr>
<td>maxpooling </td><td align="center">1.918741345 </td><td align="center">0.126148313 </td><td align="center">1.013361927 </td><td align="center">0.052107410 </td></tr>
<tr>
<td>relu </td><td align="center">0.170680821 </td><td align="center">0.019997066 </td><td align="center">0.136701009 </td><td align="center">0.014832921 </td></tr>
<tr>
<td>softmaxcrossentropy</td><td align="center">0.034060795 </td><td align="center">0.007438810 </td><td align="center">0.000000000 </td><td align="center">0.000000000 </td></tr>
</table>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
