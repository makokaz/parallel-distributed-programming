\hypertarget{relu_8h}{}\section{/home/tau/public\+\_\+html/lecture/parallel\+\_\+distributed/2018/handson/tau/parallel-\/distributed-\/handson/20vgg/include/relu.h File Reference}
\label{relu_8h}\index{/home/tau/public\+\_\+html/lecture/parallel\+\_\+distributed/2018/handson/tau/parallel-\/distributed-\/handson/20vgg/include/relu.\+h@{/home/tau/public\+\_\+html/lecture/parallel\+\_\+distributed/2018/handson/tau/parallel-\/distributed-\/handson/20vgg/include/relu.\+h}}


rectified linear activation layer (relu(x) = max(0,x))  


{\ttfamily \#include \char`\"{}vgg\+\_\+util.\+h\char`\"{}}\newline
{\ttfamily \#include \char`\"{}vgg\+\_\+arrays.\+h\char`\"{}}\newline
Include dependency graph for relu.\+h\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{relu_8h__incl}
\end{center}
\end{figure}
This graph shows which files directly or indirectly include this file\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=246pt]{relu_8h__dep__incl}
\end{center}
\end{figure}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
struct \hyperlink{structRelu}{Relu$<$ max\+B, C, H, W $>$}
\begin{DoxyCompactList}\small\item\em dropout layer \end{DoxyCompactList}\item 
struct \hyperlink{structRelu}{Relu$<$ max\+B, C, H, W $>$}
\begin{DoxyCompactList}\small\item\em dropout layer \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ }\\\+\_\+\+\_\+global\+\_\+\+\_\+ void \hyperlink{relu_8h_ab66f883c0c48ee470eea00e1e312b4b7}{forward\+\_\+global} (\hyperlink{structRelu}{Relu}$<$ maxB, C, H, W $>$ $\ast$dev, \hyperlink{structarray4}{array4}$<$ maxB, C, H, W $>$ $\ast$x\+\_\+dev)
\begin{DoxyCompactList}\small\item\em a global C\+U\+DA function that implements the baseline forward function for G\+PU \end{DoxyCompactList}\item 
{\footnotesize template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ }\\\+\_\+\+\_\+global\+\_\+\+\_\+ void \hyperlink{relu_8h_a6f9d52164079bacd6aad2e984fa68de8}{backward\+\_\+global} (\hyperlink{structRelu}{Relu}$<$ maxB, C, H, W $>$ $\ast$dev, \hyperlink{structarray4}{array4}$<$ maxB, C, H, W $>$ $\ast$gy\+\_\+dev)
\begin{DoxyCompactList}\small\item\em a global C\+U\+DA function that implements the baseline backward function for G\+PU \end{DoxyCompactList}\item 
{\footnotesize template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ }\\static \hyperlink{vgg__util_8h_a1082d08aaa761215ec83e7149f27ad16}{real} \hyperlink{relu_8h_ac2d23da00a0aabbd4cfde6833ba431c3}{relu\+\_\+grad\+\_\+check\+\_\+rand} (\hyperlink{structcmdline__opt}{cmdline\+\_\+opt} opt, \hyperlink{structlogger}{logger} $\ast$lgr, \hyperlink{structrnd__gen__t}{rnd\+\_\+gen\+\_\+t} \&rg, \hyperlink{vgg__util_8h_a8e93478a00e685bea5e6a3f617bf03a3}{idx\+\_\+t} B)
\begin{DoxyCompactList}\small\item\em check the gradient computation of a relu layer \end{DoxyCompactList}\item 
int \hyperlink{relu_8h_a4799d44fc9419d0a9259cb57d45995a9}{relu\+\_\+main} (int argc, char $\ast$$\ast$argv)
\begin{DoxyCompactList}\small\item\em entry point of this header file \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
rectified linear activation layer (relu(x) = max(0,x)) 



\subsection{Function Documentation}
\mbox{\Hypertarget{relu_8h_a6f9d52164079bacd6aad2e984fa68de8}\label{relu_8h_a6f9d52164079bacd6aad2e984fa68de8}} 
\index{relu.\+h@{relu.\+h}!backward\+\_\+global@{backward\+\_\+global}}
\index{backward\+\_\+global@{backward\+\_\+global}!relu.\+h@{relu.\+h}}
\subsubsection{\texorpdfstring{backward\+\_\+global()}{backward\_global()}}
{\footnotesize\ttfamily template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ \\
\+\_\+\+\_\+global\+\_\+\+\_\+ void backward\+\_\+global (\begin{DoxyParamCaption}\item[{\hyperlink{structRelu}{Relu}$<$ maxB, C, H, W $>$ $\ast$}]{dev,  }\item[{\hyperlink{structarray4}{array4}$<$ maxB, C, H, W $>$ $\ast$}]{gy\+\_\+dev }\end{DoxyParamCaption})}



a global C\+U\+DA function that implements the baseline backward function for G\+PU 


\begin{DoxyParams}{Parameters}
{\em (dev)} & the address of the device shadow of the object \\
\hline
{\em (gy\+\_\+dev)} & the address of the device shadow of the input matrix \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
backward\+\_\+dev 

backward\+\_\+gpu 
\end{DoxySeeAlso}
\mbox{\Hypertarget{relu_8h_ab66f883c0c48ee470eea00e1e312b4b7}\label{relu_8h_ab66f883c0c48ee470eea00e1e312b4b7}} 
\index{relu.\+h@{relu.\+h}!forward\+\_\+global@{forward\+\_\+global}}
\index{forward\+\_\+global@{forward\+\_\+global}!relu.\+h@{relu.\+h}}
\subsubsection{\texorpdfstring{forward\+\_\+global()}{forward\_global()}}
{\footnotesize\ttfamily template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ \\
\+\_\+\+\_\+global\+\_\+\+\_\+ void forward\+\_\+global (\begin{DoxyParamCaption}\item[{\hyperlink{structRelu}{Relu}$<$ maxB, C, H, W $>$ $\ast$}]{dev,  }\item[{\hyperlink{structarray4}{array4}$<$ maxB, C, H, W $>$ $\ast$}]{x\+\_\+dev }\end{DoxyParamCaption})}



a global C\+U\+DA function that implements the baseline forward function for G\+PU 


\begin{DoxyParams}{Parameters}
{\em (dev)} & the address of the device shadow of the object \\
\hline
{\em (x\+\_\+dev)} & the address of the device shadow of the input matrix \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
forward\+\_\+dev 

forward\+\_\+gpu 
\end{DoxySeeAlso}
\mbox{\Hypertarget{relu_8h_ac2d23da00a0aabbd4cfde6833ba431c3}\label{relu_8h_ac2d23da00a0aabbd4cfde6833ba431c3}} 
\index{relu.\+h@{relu.\+h}!relu\+\_\+grad\+\_\+check\+\_\+rand@{relu\+\_\+grad\+\_\+check\+\_\+rand}}
\index{relu\+\_\+grad\+\_\+check\+\_\+rand@{relu\+\_\+grad\+\_\+check\+\_\+rand}!relu.\+h@{relu.\+h}}
\subsubsection{\texorpdfstring{relu\+\_\+grad\+\_\+check\+\_\+rand()}{relu\_grad\_check\_rand()}}
{\footnotesize\ttfamily template$<$idx\+\_\+t maxB, idx\+\_\+t C, idx\+\_\+t H, idx\+\_\+t W$>$ \\
static \hyperlink{vgg__util_8h_a1082d08aaa761215ec83e7149f27ad16}{real} relu\+\_\+grad\+\_\+check\+\_\+rand (\begin{DoxyParamCaption}\item[{\hyperlink{structcmdline__opt}{cmdline\+\_\+opt}}]{opt,  }\item[{\hyperlink{structlogger}{logger} $\ast$}]{lgr,  }\item[{\hyperlink{structrnd__gen__t}{rnd\+\_\+gen\+\_\+t} \&}]{rg,  }\item[{\hyperlink{vgg__util_8h_a8e93478a00e685bea5e6a3f617bf03a3}{idx\+\_\+t}}]{B }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}



check the gradient computation of a relu layer 


\begin{DoxyParams}{Parameters}
{\em (opt)} & command line option \\
\hline
{\em (lgr)} & logger \\
\hline
{\em (rg)} & random number generator \\
\hline
{\em (\+B)} & the number of images \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
\hyperlink{relu_8h_a4799d44fc9419d0a9259cb57d45995a9}{relu\+\_\+main}
\end{DoxySeeAlso}
it first makes a layer object with initial weights W and generates an input (x and t). it then creates two layers whose weights are slightly different from the original one by dw/2 (i.\+e., w-\/dw/2 and w+dw/2), as well as two inputs slighly different from the original inputs by dx/2 (x-\/dx/2 and x+dx/2). it then computes L(w,x), L(x-\/dw/2,x-\/dx/2) and L(w+dw/2,x+dw/2) and check if L(x+dw/2,x+dx/2)-\/L(x-\/dw/2,x-\/dx/2) is close to ∂\+L/∂x dx + ∂\+L/∂w dw. ∂\+L/∂x and ∂\+L/∂w are obtained by backward computation. This is essentially checking if the gradients obtained by backward computation correctly approximates the diff of the output. \mbox{\Hypertarget{relu_8h_a4799d44fc9419d0a9259cb57d45995a9}\label{relu_8h_a4799d44fc9419d0a9259cb57d45995a9}} 
\index{relu.\+h@{relu.\+h}!relu\+\_\+main@{relu\+\_\+main}}
\index{relu\+\_\+main@{relu\+\_\+main}!relu.\+h@{relu.\+h}}
\subsubsection{\texorpdfstring{relu\+\_\+main()}{relu\_main()}}
{\footnotesize\ttfamily int relu\+\_\+main (\begin{DoxyParamCaption}\item[{int}]{argc,  }\item[{char $\ast$$\ast$}]{argv }\end{DoxyParamCaption})}



entry point of this header file 


\begin{DoxyParams}{Parameters}
{\em (argc)} & the number of command line args \\
\hline
{\em (argv)} & command line args \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
\hyperlink{relu_8h_ac2d23da00a0aabbd4cfde6833ba431c3}{relu\+\_\+grad\+\_\+check\+\_\+rand}
\end{DoxySeeAlso}
if this header file is included from a main C++ file and define relu\+\_\+main to be main (e.\+g., with -\/\+Drelu\+\_\+main=main), then this function becomes th main function of the executable. it calls relu\+\_\+grad\+\_\+check\+\_\+rand repeatedly to test the implementation of \hyperlink{structVGG}{V\+GG} network. 